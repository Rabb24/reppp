{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "def get_top_words(file_path, title_column, top_n=50):\n",
        "    # Wczytanie danych z pliku CSV\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Złączenie wszystkich tytułów w jeden tekst\n",
        "    all_titles = ' '.join(df[title_column].dropna().astype(str).tolist())\n",
        "\n",
        "    # Usunięcie znaków interpunkcyjnych i podzielenie na słowa, pomijanie liczb\n",
        "    words = re.findall(r'\\b[a-zA-Z]+\\b', all_titles.lower())\n",
        "\n",
        "    # Lista dodatkowych stop words\n",
        "    additional_stop_words = {'a', 'i', 'na', 'do', 'za', 'z'}\n",
        "\n",
        "    # Wczytanie standardowych stop words z NLTK i dodanie dodatkowych\n",
        "    from nltk.corpus import stopwords\n",
        "    import nltk\n",
        "    nltk.download('stopwords')\n",
        "    stop_words = set(stopwords.words('english')).union(additional_stop_words)\n",
        "\n",
        "    # Filtrowanie słów, aby usunąć stop words\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Zliczanie najczęściej występujących słów\n",
        "    word_counts = Counter(filtered_words)\n",
        "    most_common_words = word_counts.most_common(top_n)\n",
        "\n",
        "    return most_common_words\n",
        "\n",
        "# Ścieżka do pliku CSV i nazwa kolumny z tytułami\n",
        "file_path = 'path_to_your_file.csv'\n",
        "title_column = 'title'\n",
        "\n",
        "# Wywołanie funkcji i wyświetlenie wyników\n",
        "top_words = get_top_words(file_path, title_column)\n",
        "print(top_words)\n"
      ],
      "metadata": {
        "id": "o9_BFrZGU5pO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Pobranie stop words z NLTK, w tym przypadku dla języka polskiego\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def get_polish_stop_words():\n",
        "    # Rozszerzenie listy polskich stop words\n",
        "    additional_stop_words = {'a', 'i', 'na', 'do', 'za', 'z', 'oraz', 'ale', 'bądź', 'bo', 'by', 'być', 'ci', 'co', 'czy',\n",
        "                             'dla', 'gdy', 'jak', 'jeśli', 'jest', 'to', 'już', 'który', 'lub', 'nie', 'o', 'po', 'przez',\n",
        "                             'się', 'tak', 'tu', 'więc', 'że', 'aż', 'czyli', 'gdyby', 'jakie', 'jego', 'jej', 'są', 'ich',\n",
        "                             'jeżeli', 'jako', 'jego', 'jej', 'się', 'nad', 'u', 'pod', 'przed', 'we', 'bez', 'od', 'między'}\n",
        "    stop_words = set(stopwords.words('polish')).union(additional_stop_words)\n",
        "    return stop_words\n",
        "\n",
        "def get_top_words_nltk(file_path, title_column, top_n=50):\n",
        "    # Wczytanie danych z pliku CSV\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Złączenie wszystkich tytułów w jeden tekst\n",
        "    all_titles = ' '.join(df[title_column].dropna().astype(str).tolist())\n",
        "\n",
        "    # Usunięcie znaków interpunkcyjnych i podzielenie na słowa\n",
        "    words = re.findall(r'\\b\\w+\\b', all_titles.lower())\n",
        "\n",
        "    # Usunięcie stop words\n",
        "    stop_words = get_polish_stop_words()\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Zliczanie najczęściej występujących słów\n",
        "    word_counts = Counter(filtered_words)\n",
        "    most_common_words = word_counts.most_common(top_n)\n",
        "\n",
        "    return most_common_words\n",
        "\n",
        "# Ścieżka do pliku CSV i nazwa kolumny z tytułami\n",
        "file_path = 'path_to_your_file.csv'\n",
        "title_column = 'title'\n",
        "\n",
        "# Wywołanie funkcji i wyświetlenie wyników\n",
        "top_words = get_top_words_nltk(file_path, title_column)\n",
        "print(top_words)\n"
      ],
      "metadata": {
        "id": "aSek6j1sU5hZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "\n",
        "def get_polish_stop_words():\n",
        "    # Lista polskich stop words, można ją rozszerzyć w razie potrzeby\n",
        "    polish_stop_words = set([\n",
        "        'i', 'oraz', 'a', 'ale', 'bądź', 'bo', 'by', 'być', 'ci', 'co', 'czy',\n",
        "        'dla', 'do', 'gdy', 'jak', 'jeśli', 'jest', 'to', 'już', 'który', 'lub',\n",
        "        'na', 'nie', 'o', 'po', 'przez', 'się', 'tak', 'to', 'tu', 'więc',\n",
        "        'z', 'że', 'aż', 'czyli', 'gdyby', 'jakie', 'jako', 'jego', 'jej', 'jest', 'są',\n",
        "        'ich', 'jeżeli', 'jako', 'jego', 'jej', 'się', 'na', 'jest', 'być', 'co', 'do', 'za',\n",
        "        'z', 'o', 'ze', 'nad', 'u', 'pod', 'po', 'przed', 'we', 'bez', 'dla', 'w', 'od', 'między'\n",
        "    ])\n",
        "    return polish_stop_words\n",
        "\n",
        "def get_top_words_sklearn(file_path, title_column, top_n=50):\n",
        "    # Wczytanie danych z pliku CSV\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Złączenie wszystkich tytułów w jeden tekst\n",
        "    all_titles = df[title_column].dropna().astype(str).tolist()\n",
        "\n",
        "    # Użycie CountVectorizer do tokenizacji i zliczania słów\n",
        "    stop_words = get_polish_stop_words()\n",
        "    vectorizer = CountVectorizer(stop_words=stop_words)\n",
        "    X = vectorizer.fit_transform(all_titles)\n",
        "\n",
        "    # Zliczanie wystąpień słów\n",
        "    word_counts = X.toarray().sum(axis=0)\n",
        "    word_freq = [(word, word_counts[idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
        "    word_freq = sorted(word_freq, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Pobieranie top_n słów\n",
        "    most_common_words = word_freq[:top_n]\n",
        "\n",
        "    return most_common_words\n",
        "\n",
        "# Ścieżka do pliku CSV i nazwa kolumny z tytułami\n",
        "file_path = 'path_to_your_file.csv'\n",
        "title_column = 'title'\n",
        "\n",
        "# Wywołanie funkcji i wyświetlenie wyników\n",
        "top_words = get_top_words_sklearn(file_path, title_column)\n",
        "print(top_words)\n"
      ],
      "metadata": {
        "id": "MYP9zKJMU5Xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "# Załadowanie modelu spaCy dla języka polskiego\n",
        "nlp = spacy.load('pl_core_news_sm')\n",
        "\n",
        "def get_top_words_spacy(file_path, title_column, top_n=50):\n",
        "    # Wczytanie danych z pliku CSV\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Złączenie wszystkich tytułów w jeden tekst\n",
        "    all_titles = ' '.join(df[title_column].dropna().astype(str).tolist())\n",
        "\n",
        "    # Przetwarzanie tekstu za pomocą spaCy\n",
        "    doc = nlp(all_titles)\n",
        "\n",
        "    # Tokenizacja i usunięcie stop words\n",
        "    words = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "    # Zliczanie najczęściej występujących słów\n",
        "    word_counts = Counter(words)\n",
        "    most_common_words = word_counts.most_common(top_n)\n",
        "\n",
        "    return most_common_words\n",
        "\n",
        "# Ścieżka do pliku CSV i nazwa kolumny z tytułami\n",
        "file_path = 'path_to_your_file.csv'\n",
        "title_column = 'title'\n",
        "\n",
        "# Wywołanie funkcji i wyświetlenie wyników\n",
        "top_words = get_top_words_spacy(file_path, title_column)\n",
        "print(top_words)\n"
      ],
      "metadata": {
        "id": "8RnUqNFXYtE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8L0xqrF-Ys8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iXatAnpFYs43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kAuL5uclYs2E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}